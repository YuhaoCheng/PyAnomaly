import numpy as np
import os
import pickle
import torch
from collections import OrderedDict
from ..abstract.readers import GroundTruthLoader
from scipy.ndimage import gaussian_filter1d


def load_pickle_results(loss_file, cfg):
    with open(loss_file, 'rb') as f:
        # results {
        #   'dataset': the name of dataset
        #   'psnr': the psnr of each testing videos,
        #   'flow': [], 
        #   'names': [], 
        #   'diff_mask': [], 
        #   'score': the score of each testing videos
        #   'num_videos': the number of the videos
        # }

        # psnr_records['psnr'] is np.array, shape(#videos)
        # psnr_records[0] is np.array   ------>     01.avi
        # psnr_records[1] is np.array   ------>     02.avi
        #               ......
        # psnr_records[n] is np.array   ------>     xx.avi

        results = pickle.load(f)

    dataset = results['dataset']
    # psnr_records = results['psnr']
    # score_records = results['score']
    score_records = list()
    psnr_records = list()
    num_videos = results['num_videos']
    # import ipdb; ipdb.set_trace()
    if cfg.DATASET.smooth.guassian:
        for sigma in cfg.DATASET.smooth.guassian_sigma:
            # score_records = results[f'score_smooth_{sigma}']
            # psnr_records = results[f'psnr_smooth_{sigma}']
            score_records.append(results[f'score_smooth_{sigma}'])
            if len(results['psnr']) == 0:
                psnr_records = [results[f'psnr_smooth_{cfg.DATASET.smooth.guassian_sigma[0]}']]
            else:
                psnr_records.append(results[f'psnr_smooth_{sigma}'])
        # new_score = []
        # for index, item in enumerate(score):
        #     temp = gaussian_filter1d(score[index], cfg.DATASET.smooth.guassian_sigma)
        #     new_score.append(temp)
        # print(f'Smooth the score with sigma:{cfg.DATASET.smooth.guassian_sigma}')
    else:
        # score_records = results['score']
        # psnr_records = results['psnr']
        score_records.append(results['score'])
        psnr_records.append(results['psnr'])
    # score = np.array(new_score)
    assert dataset == cfg.DATASET.name, f'The dataset are not match, Result:{dataset}, cfg:{cfg.DATASET.name}'

    # load ground truth
    gt_loader = GroundTruthLoader(cfg)
    # gt = gt_loader(dataset=dataset)
    # gt = gt_loader()
    gt = gt_loader.read(cfg.DATASET.name, cfg.DATASET.gt_path, cfg.DATASET.test_path) # because this method is only used for test/val

    assert num_videos == len(gt), f'the number of saved videos does not match the ground truth, {num_videos} != {len(gt)}' 

    return dataset, psnr_records, score_records, gt, num_videos


def psnr_error(gen_frames, gt_frames, hat=False):
    """
    Computes the Peak Signal to Noise Ratio error between the generated images and the ground
    truth images.
    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the
                       generator model.
    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for
                      each frame in gen_frames.
    @return: A scalar tensor. The mean Peak Signal to Noise Ratio error over each frame in the
             batch.
    """
    gen_frames = gen_frames.detach().cpu()
    gt_frames = gt_frames.detach().cpu()
    batch_num = gen_frames.shape[0]
    batch_errors = 0.0
    for i in range(batch_num):
        num_pixels = gen_frames[i].numel()
        # max_val_hat = gen_frames[i].max()
        if hat:
            max_val = gen_frames[i].max()
        else:
            max_val = gt_frames[i].max()
        square_diff = (gt_frames[i] - gen_frames[i])**2
        log_value = torch.log10(max_val ** 2 / ((1. / num_pixels) * torch.sum(square_diff)))
        image_errors = 10 * log_value
        batch_errors += image_errors
    
    batch_errors = torch.div(batch_errors, batch_num)
    return batch_errors

def simple_diff(frame_true, frame_hat, flow_true, flow_hat, aggregation=False):
    """
    """
    assert frame_true.shape == frame_hat.shape
    assert flow_true.shape == flow_hat.shape

    frame_true = frame_true.squeeze(0).detach()
    frame_hat = frame_hat.squeeze(0).detach()
    flow_true = flow_true.squeeze(0).detach()
    flow_hat = flow_hat.squeeze(0).detach()

    loss_appe = (frame_true-frame_hat)**2
    loss_flow = (flow_true-flow_hat)**2

    if aggregation:
        loss_appe = torch.mean(loss_appe)
        loss_flow = torch.mean(loss_flow)

    return loss_appe, loss_flow

def find_max_patch(diff_map_appe, diff_map_flow, kernel_size=16, stride=4, aggregation=True):
    '''
    kernel size = window size
    '''
    # max_pool = torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride)
    avg_pool = torch.nn.AvgPool2d(kernel_size=kernel_size, stride=stride)
    max_patch_appe = avg_pool(diff_map_appe)
    max_patch_flow = avg_pool(diff_map_flow)
    # import ipdb; ipdb.set_trace()
    assert len(max_patch_appe.shape) == 4, f'the shape of max_patch_appe is {max_patch_appe.shape}'
    # assert len(max_patch_appe.shape) == 3, f'the shape of max_patch_appe is {max_patch_appe.shape}'
    assert len(max_patch_flow.shape) == 4, f'the shape of max_patch_flow is {max_patch_flow.shape}'
    # assert len(max_patch_flow.shape) == 3, f'the shape of max_patch_flow is {max_patch_flow.shape}'

    if aggregation:
        # Will sum the channel dim
        # max_patch_appe = torch.mean(max_patch_appe, dim=0) 
        max_patch_appe = torch.mean(max_patch_appe, dim=1) 
        max_patch_flow = torch.mean(max_patch_flow, dim=1)
        # max_patch_flow = torch.mean(max_patch_flow, dim=0)

    max_appe_value = torch.max(max_patch_appe)
    max_flow_value = torch.max(max_patch_flow)
    
    # max_val_flow_std = 0.0
    # max_val_appe_std = 0.0
    # pos_flow_std = [0, 0]
    # pos_appe_std = [0, 0]

    # for i in range(0, diff_map_flow.shape[0]-kernel_size, stride):
    #     for j in range(0, diff_map_flow.shape[1]-kernel_size, stride):
    #         curr_std_flow = torch.std(diff_map_flow[i:i+kernel_size, j:j+kernel_size])
    #         # curr_mean = np.mean(diff_map_flow[i:i+kernel_size, j:j+kernel_size])
    #         # curr_std_appe = torch.std(diff_map_appe[i:i+kernel_size, j:j+kernel_size])
    #         # curr_mean_appe = np.mean(diff_map_appe[i:i+kernel_size, j:j+kernel_size])
    #         # if curr_mean > max_val_mean:
    #         #     max_val_mean = curr_mean
    #         #     std_1 = curr_std
    #         #     pos_1 = [i, j]
    #         #     std_appe_1 = curr_std_appe
    #         #     mean_appe_1 = curr_mean_appe
    #         if curr_std_flow > max_val_flow_std:
    #             max_val_flow_std = curr_std_flow
    #             # mean_2 = curr_mean
    #             pos_flow_std = [i, j]
    #             # std_appe_2 = curr_std_appe
    #             # mean_appe_2 = curr_mean_appe
    
    # for i in range(0, diff_map_appe.shape[0]-kernel_size, stride):
    #     for j in range(0, diff_map_appe.shape[1]-kernel_size, stride):
    #         # curr_std_flow = torch.std(diff_map_flow[i:i+kernel_size, j:j+kernel_size])
    #         # curr_mean = np.mean(diff_map_flow[i:i+kernel_size, j:j+kernel_size])
    #         curr_std_appe = torch.std(diff_map_appe[i:i+kernel_size, j:j+kernel_size])
    #         # curr_mean_appe = np.mean(diff_map_appe[i:i+kernel_size, j:j+kernel_size])
    #         # if curr_mean > max_val_mean:
    #         #     max_val_mean = curr_mean
    #         #     std_1 = curr_std
    #         #     pos_1 = [i, j]
    #         #     std_appe_1 = curr_std_appe
    #         #     mean_appe_1 = curr_mean_appe
    #         if curr_std_appe > max_val_appe_std:
    #             max_val_appe_std = curr_std_appe
    #             # mean_2 = curr_mean
    #             pos_appe_std = [i, j]
    #             # std_appe_2 = curr_std_appe
    #             # mean_appe_2 = curr_mean_appe
    
    # app_h, app_w =  torch.where(torch.eq(max_patch_appe, max_appe_value))
    # flow_h, flow_w =  torch.where(torch.eq(max_patch_flow, max_flow_value))
    
    max_appe_final = max_appe_value
    max_flow_final = max_flow_value
    # max_appe_final = torch.div(max_appe_value, kernel_size**2)
    # max_flow_final = torch.div(max_flow_value, kernel_size**2) 
    # import ipdb; ipdb.set_trace()
    # return max_patch_appe, max_patch_flow
    # return max_appe_final, max_flow_final, (app_h, app_w), (flow_h, flow_w)
    return max_appe_final, max_flow_final 
    # return max_val_appe_std, max_val_flow_std, (app_h, app_w), (flow_h, flow_w)

def calc_w(w_dict):
    wf = 0.0
    wi = 0.0
    n = 0
    for key in w_dict.keys():
        # n += w_dict[key][0]
        n += 1
        wf += w_dict[key][1]
        wi += w_dict[key][2]
    # import ipdb; ipdb.set_trace()
    wf = torch.div(1.0, torch.div(wf, n))
    wi = torch.div(1.0, torch.div(wi, n))

    return wf, wi

def amc_normal_score(wf, sf, wi, si, lambada_s=0.2):
    final_score = torch.log(wf * sf) + lambada_s * torch.log(wi*si)

    return final_score

def amc_score(frame, frame_hat, flow, flow_hat, wf, wi, kernel_size=16, stride=4, lambada_s=0.2):
    '''
    wf, wi is different from videos
    '''
    loss_appe, loss_flow = simple_diff(frame, frame_hat, flow, flow_hat)
    max_patch_appe, max_patch_flow, app_cord, flow_crod = find_max_patch(loss_appe, loss_flow, kernel_size=kernel_size, stride=stride)
    final_score = amc_normal_score(wf, max_patch_appe, wi, max_patch_flow, lambada_s=lambada_s)

    return final_score, app_cord, flow_crod

def oc_score(raw_data):
    object_score = np.empty(shape=(raw_data.shape[0],),dtype=np.float32)
    for index, dummy_objects in enumerate(raw_data):
        # temp = np.max(-dummy_objects)
        temp = -np.max(dummy_objects)
        object_score[index] = temp
    
    frame_score = np.max(object_score)
    # import ipdb; ipdb.set_trace()
    return frame_score


def reconstruction_loss(x_hat, x):
    '''
    The input is the video clip, and we use the RL as the score.
    RL := Reconstruction Loss
    '''
    x_hat = x_hat.squeeze(0).detach()
    x = x.squeeze(0).detach()
    rl = torch.sqrt(torch.pow((x_hat - x), 2))
    h_dim = len(rl.shape) - 2
    w_dim = len(rl.shape) - 1
    # the number of rl is equal to the frame number
    rl = torch.mean(rl, (h_dim, w_dim)).squeeze(0)
    if len(rl.shape) == 2:
        rl = rl.mean(0)
    elif len(rl.shape) == 1:
        rl = rl
    else:
        raise Exception('in reconstruction_loss')
    # import ipdb; ipdb.set_trace()
    return rl

def get_scores_labels(loss_file, cfg):
    '''
    base the psnr to get the scores of each videos
    '''
    # the name of dataset, loss, and ground truth
    dataset, psnr_records, _, gt, _ = load_pickle_results(loss_file=loss_file, cfg=cfg)

    # the number of videos
    num_videos = len(psnr_records)

    # how many frames to ignore at first
    DECIDABLE_IDX = cfg.DATASET.decidable_idx

    scores = np.array([], dtype=np.float32)
    labels = np.array([], dtype=np.int8)
    # video normalization
    for i in range(num_videos):
        psnr_single_video = psnr_records[i]
        psnr_min = psnr_single_video.min()
        psnr_max = psnr_single_video.max()

        if cfg.DATASET.score_normalize:
            psnr_single_video -= psnr_min  # distances = (distance - min) / (max - min)
            psnr_single_video /= psnr_max
            # distance = 1 - distance

        scores = np.concatenate((scores[:], psnr_single_video[DECIDABLE_IDX:]), axis=0)
        labels = np.concatenate((labels[:], gt[i][DECIDABLE_IDX:]), axis=0)
    return dataset, scores, labels


def precision_recall_auc(loss_file, cfg):
    if not os.path.isdir(loss_file):
        loss_file_list = [loss_file]
    else:
        loss_file_list = os.listdir(loss_file)
        loss_file_list = [os.path.join(loss_file, sub_loss_file) for sub_loss_file in loss_file_list]

    optimal_results = RecordResult()
    for sub_loss_file in loss_file_list:
        dataset, scores, labels = get_scores_labels(loss_file=sub_loss_file, cfg=cfg)
        precision, recall, thresholds = metrics.precision_recall_curve(labels, scores, pos_label=0)
        auc = metrics.auc(recall, precision)

        results = RecordResult(recall, precision, thresholds, auc, dataset, sub_loss_file)

        if optimal_results < results:
            optimal_results = results

        if os.path.isdir(loss_file):
            print(results)
    print('##### optimal result and model = {}'.format(optimal_results))
    return optimal_results


def cal_eer(fpr, tpr):
    # makes fpr + tpr = 1
    eer = fpr[np.nanargmin(np.absolute((fpr + tpr - 1)))]
    return eer


def compute_eer(loss_file, cfg):
    if not os.path.isdir(loss_file):
        loss_file_list = [loss_file]
    else:
        loss_file_list = os.listdir(loss_file)
        loss_file_list = [os.path.join(loss_file, sub_loss_file) for sub_loss_file in loss_file_list]

    optimal_results = RecordResult(auc=np.inf)
    for sub_loss_file in loss_file_list:
        dataset, scores, labels = get_scores_labels(loss_file=sub_loss_file, cfg=cfg)
        fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=0)
        eer = cal_eer(fpr, tpr)

        results = RecordResult(fpr, tpr, thresholds, eer, dataset, sub_loss_file)

        if optimal_results > results:
            optimal_results = results

        if os.path.isdir(loss_file):
            print(results)
    print('##### optimal result and model = {}'.format(optimal_results))
    return optimal_results

def average_psnr(loss_file, cfg):
    if not os.path.isdir(loss_file):
        loss_file_list = [loss_file]
    else:
        loss_file_list = os.listdir(loss_file)
        loss_file_list = [os.path.join(loss_file, sub_loss_file) for sub_loss_file in loss_file_list]

    max_avg_psnr = -np.inf
    max_file = ''
    for file in loss_file_list:
        _, psnr_records, _, gt, _ = load_pickle_results(file, cfg)

        psnr_records = np.concatenate(psnr_records, axis=0)
        avg_psnr = np.mean(psnr_records)
        if max_avg_psnr < avg_psnr:
            max_avg_psnr = avg_psnr
            max_file = file
        print('{}, average psnr = {}'.format(file, avg_psnr))

    print('max average psnr file = {}, psnr = {}'.format(max_file, max_avg_psnr))


def calculate_psnr(loss_file, logger, cfg):
    optical_result = compute_auc_score(loss_file, logger, cfg)
    print('##### optimal result and model = {}'.format(optical_result))

    mean_psnr = []
    for file in os.listdir(loss_file):
        file = os.path.join(loss_file, file)
        dataset, psnr_records, _, gt, _ = load_pickle_results(file, cfg)

        psnr_records = np.concatenate(psnr_records, axis=0)
        gt = np.concatenate(gt, axis=0)

        mean_normal_psnr = np.mean(psnr_records[gt == 0])
        mean_abnormal_psnr = np.mean(psnr_records[gt == 1])
        mean = np.mean(psnr_records)
        print('mean normal psrn = {}, mean abnormal psrn = {}, mean = {}'.format(
            mean_normal_psnr,
            mean_abnormal_psnr,
            mean)
        )
        mean_psnr.append(mean)
    print('max mean psnr = {}'.format(np.max(mean_psnr)))


def compute_auc_psnr(loss_file, logger, cfg, score_type='normal'):
    '''
    For psnr, score_type is always 'normal', means that the higher PSNR, the higher normality 
    '''
    if not os.path.isdir(loss_file):
        loss_file_list = [loss_file]
    else:
        loss_file_list = os.listdir(loss_file)
        loss_file_list = [os.path.join(loss_file, sub_loss_file) for sub_loss_file in loss_file_list]

    optimal_results = RecordResult()
    DECIDABLE_IDX = cfg.DATASET.decidable_idx
    for sub_loss_file in loss_file_list:
        # the name of dataset, loss, and ground truth
        dataset, psnr_records, score_records, gt, num_videos = load_pickle_results(loss_file=sub_loss_file, cfg=cfg)

        # the number of videos
        assert num_videos == len(score_records), 'The num of video is not equal'

        scores = np.array([], dtype=np.float32)
        labels = np.array([], dtype=np.int8)
        
        # video normalization
        for i in range(num_videos):
            distance = psnr_records[i]
            scores = np.concatenate((scores, distance[DECIDABLE_IDX:]), axis=0)
            labels = np.concatenate((labels, gt[i][DECIDABLE_IDX:]), axis=0)
        '''
        Normalization is in the process of calculate the scores, instead of beforing getting the AUC
        '''
        # if cfg.DATASET.score_normalize:
        #     smin = scores.min()
        #     smax = scores.max()
        #     scores = scores - scores.min()  # scores = (scores - min) / (max - min)
        #     scores = scores / (smax - smin)
        fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=0)
        auc = metrics.auc(fpr, tpr)

        results = RecordResult(fpr, tpr, thresholds, auc, dataset, sub_loss_file)

        if optimal_results < results:
            optimal_results = results

        if os.path.isdir(loss_file):
            print(results)
    logger.info(f'##### optimal result and model = {optimal_results}')
    return optimal_results


def compute_auc_score(loss_file, logger, cfg, score_type='normal'):
    '''
    score_type:
        normal--> pos_label=0
        abnormal --> pos_label=1
        in dataset, 0 means normal, 1 means abnormal
    '''
    def get_results(score_record, sigma, pos_label):
        scores = np.array([], dtype=np.float32)
        labels = np.array([], dtype=np.int8)

        # video normalization
        for i in range(num_videos):
            score_one_video = score_record[i]
            l = len(score_one_video)
            score_one_video = np.clip(score_one_video, 0, None)
            scores = np.concatenate((scores, score_one_video[DECIDABLE_IDX:l-DECIDABLE_IDX_BACK]), axis=0)
            labels = np.concatenate((labels, gt[i][DECIDABLE_IDX:l-DECIDABLE_IDX_BACK]), axis=0)
        
        fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=pos_label)
        auc = metrics.auc(fpr, tpr)
        results = RecordResult(fpr, tpr, thresholds, auc, dataset, sub_loss_file, sigma)

        return results
        
    if score_type == 'normal':
        pos_label = 0
    elif score_type == 'abnormal':
        pos_label =1
    else:
        raise Exception('Error in score_type')
    
    if not os.path.isdir(loss_file):
        loss_file_list = [loss_file]
    else:
        loss_file_list = os.listdir(loss_file)
        loss_file_list = [os.path.join(loss_file, sub_loss_file) for sub_loss_file in loss_file_list]

    optimal_results = RecordResult()
    DECIDABLE_IDX = cfg.DATASET.decidable_idx
    DECIDABLE_IDX_BACK = cfg.DATASET.decidable_idx
    for sub_loss_file in loss_file_list:
        # the name of dataset, loss, and ground truth
        dataset, psnr_records, score_records, gt, num_videos = load_pickle_results(loss_file=sub_loss_file, cfg=cfg)

        assert num_videos == len(score_records[0]), 'The num of video is not equal'

        # scores = np.array([], dtype=np.float32)
        # labels = np.array([], dtype=np.int8)
        
        # # video normalization
        # for i in range(num_videos):
        #     score_one_video = score_records[i]
        #     score_one_video = np.clip(score_one_video, 0, None)
        #     scores = np.concatenate((scores, score_one_video[DECIDABLE_IDX:]), axis=0)
        #     labels = np.concatenate((labels, gt[i][DECIDABLE_IDX:]), axis=0)
        # '''
        # Normalization is in the process of calculate the scores, instead of beforing getting the AUC
        # '''
        # # if cfg.DATASET.score_normalize:
        # #     smin = scores.min()
        # #     smax = scores.max()
        # #     scores = scores - smin  # scores = (scores - min) / (max - min)
        # #     scores = scores / (smax - smin)
        # fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=pos_label)
        # auc = metrics.auc(fpr, tpr)

        # results = RecordResult(fpr, tpr, thresholds, auc, dataset, sub_loss_file)
        if cfg.DATASET.smooth.guassian:
            for index, sigma in enumerate(cfg.DATASET.smooth.guassian_sigma):
                score_record = score_records[index]
                results = get_results(score_record, sigma, pos_label)
                if optimal_results < results:
                    optimal_results = results
        else:
            results = get_results(score_records[0], 0, pos_label)
            if optimal_results < results:
                optimal_results = results

        if os.path.isdir(loss_file):
            print(results)
    logger.info(f'##### optimal result and model = {optimal_results}')
    return optimal_results

# use this dict to store the evaluate functions
# eval_functions = {
#     'compute_auc_psnr': compute_auc_psnr,
#     'compute_auc_score': compute_auc_score,
#     'compute_eer': compute_eer,
#     'precision_recall_auc': precision_recall_auc,
#     'calculate_psnr': calculate_psnr,
#     # 'calculate_score': calculate_score,
#     'average_psnr': average_psnr,
#     'average_psnr_sample': average_psnr
# }
